{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "KXeY5DZ97TNC",
        "outputId": "58d097b0-8720-4716-cbad-a6a7e7b1d320"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Boat Type Classification using CNN and MobileNetV2\n",
        "\n",
        "This notebook walks through a deep learning project for classifying boat types from images. We will build, train, and evaluate two models: a custom Convolutional Neural Network (CNN) and a transfer learning model using MobileNetV2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset successfully split into train, validation, and test sets.\n",
            "Total classes: 9\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Step 0: Split Dataset into Train, Validation, and Test sets\n",
        "base_dir = '../boat_type_classification_dataset'\n",
        "output_dir = './data'\n",
        "\n",
        "# Create directories for train, validation, and test sets\n",
        "train_dir = os.path.join(output_dir, 'train')\n",
        "validation_dir = os.path.join(output_dir, 'validation')\n",
        "test_dir = os.path.join(output_dir, 'test')\n",
        "\n",
        "if os.path.exists(output_dir):\n",
        "    shutil.rmtree(output_dir)\n",
        "\n",
        "os.makedirs(train_dir)\n",
        "os.makedirs(validation_dir)\n",
        "os.makedirs(test_dir)\n",
        "\n",
        "# Get the list of classes (boat types)\n",
        "classes = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
        "\n",
        "for cls in classes:\n",
        "    # Create subdirectories for each class in train, validation, and test\n",
        "    os.makedirs(os.path.join(train_dir, cls))\n",
        "    os.makedirs(os.path.join(validation_dir, cls))\n",
        "    os.makedirs(os.path.join(test_dir, cls))\n",
        "\n",
        "    # Get all image file names for the class\n",
        "    src_dir = os.path.join(base_dir, cls)\n",
        "    all_files = os.listdir(src_dir)\n",
        "    random.shuffle(all_files)\n",
        "\n",
        "    # Define split ratios\n",
        "    train_ratio = 0.7\n",
        "    validation_ratio = 0.15\n",
        "    test_ratio = 0.15\n",
        "\n",
        "    # Calculate split indices\n",
        "    train_split = int(len(all_files) * train_ratio)\n",
        "    validation_split = int(len(all_files) * (train_ratio + validation_ratio))\n",
        "\n",
        "    # Get file lists for each set\n",
        "    train_files = all_files[:train_split]\n",
        "    validation_files = all_files[train_split:validation_split]\n",
        "    test_files = all_files[validation_split:]\n",
        "\n",
        "    # Function to copy files\n",
        "    def copy_files(files, dest_dir):\n",
        "        for f in files:\n",
        "            shutil.copy(os.path.join(src_dir, f), os.path.join(dest_dir, cls, f))\n",
        "\n",
        "    # Copy files to their respective directories\n",
        "    copy_files(train_files, train_dir)\n",
        "    copy_files(validation_files, validation_dir)\n",
        "    copy_files(test_files, test_dir)\n",
        "\n",
        "print(\"Dataset successfully split into train, validation, and test sets.\")\n",
        "print(f\"Total classes: {len(classes)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Split the Dataset\n",
        "\n",
        "To ensure our model is evaluated correctly, we split the dataset into three parts:\n",
        "- **Training set (70%)**: Used to train the model.\n",
        "- **Validation set (15%)**: Used to tune hyperparameters and evaluate the model during training.\n",
        "- **Test set (15%)**: Used to provide an unbiased evaluation of the final model.\n",
        "\n",
        "This helps prevent data leakage and gives a more accurate measure of the model's performance on unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Import Libraries\n",
        "\n",
        "This cell imports all the necessary libraries for the project, including:\n",
        "- `numpy` for numerical operations.\n",
        "- `tensorflow` and `keras` for building and training the deep learning models.\n",
        "- `matplotlib` and `seaborn` for plotting graphs and visualizations.\n",
        "- `sklearn` for performance metrics and utility functions.\n",
        "- `os`, `shutil`, and `random` for file and directory operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final comparison\n",
        "print(f\"Custom CNN Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"MobileNetV2 Test Accuracy: {test_acc_mobilenet:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the best model (MobileNetV2) to a file\n",
        "model_save_path = 'boat_classifier_mobilenet.h5'\n",
        "mobilenet_model.save(model_save_path)\n",
        "\n",
        "print(f\"Model saved to {model_save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Save the Best Model\n",
        "\n",
        "Now that we've compared the models, we'll save the best-performing one (MobileNetV2) to a file. This will allow us to use our trained model in a separate application, like a web backend, without having to retrain it every time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Final Comparison\n",
        "\n",
        "Here, we print the final test accuracies of both models to see which one performed better on our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_history(history, title):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title(f'{title} - Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title(f'{title} - Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history_cnn, 'Custom CNN')\n",
        "plot_history(history_mobilenet, 'MobileNetV2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Compare Models and Visualize Results\n",
        "\n",
        "Finally, we'll compare the performance of the two models and visualize their training history.\n",
        "\n",
        "### 4.1 Plot Training History\n",
        "\n",
        "These plots show the model's `loss` and `accuracy` on both the training and validation sets over epochs. They are useful for diagnosing issues like overfitting (where training accuracy is much higher than validation accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on the test set\n",
        "test_loss_mobilenet, test_acc_mobilenet = mobilenet_model.evaluate(test_generator)\n",
        "print(f\"MobileNetV2 Test Accuracy: {test_acc_mobilenet:.4f}\")\n",
        "\n",
        "# Get predictions\n",
        "y_pred_mobilenet = np.argmax(mobilenet_model.predict(test_generator), axis=1)\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred_mobilenet, target_names=class_labels))\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix_mobilenet = confusion_matrix(y_true, y_pred_mobilenet)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix_mobilenet, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('MobileNetV2 Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Evaluate the MobileNetV2 Model\n",
        "\n",
        "We evaluate the MobileNetV2 model on the test set, just as we did for the CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history_mobilenet = mobilenet_model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=50,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Train the MobileNetV2 Model\n",
        "\n",
        "We train this model similarly to the CNN, but we don't need `class_weights` here, as transfer learning models are often more robust to imbalance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the base model\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False  # Freeze the base model\n",
        "\n",
        "# Build the model\n",
        "mobilenet_model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(256, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.1),\n",
        "    Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "mobilenet_model.compile(\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "mobilenet_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Build and Train MobileNetV2 Model (Transfer Learning)\n",
        "\n",
        "Next, we'll use a pre-trained model, **MobileNetV2**, for transfer learning. This approach leverages a model that has already been trained on a massive dataset (ImageNet) and adapts it for our specific task.\n",
        "\n",
        "### 3.1 Load the Pre-trained Model\n",
        "\n",
        "We load MobileNetV2 without its final classification layer (`include_top=False`) and freeze its weights (`trainable=False`) so that we only train our custom layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on the test set\n",
        "test_loss, test_acc = cnn_model.evaluate(test_generator)\n",
        "print(f\"CNN Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Get predictions\n",
        "y_pred = np.argmax(cnn_model.predict(test_generator), axis=1)\n",
        "y_true = test_generator.classes\n",
        "class_labels = list(test_generator.class_indices.keys())\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_labels))\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('CNN Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Evaluate the CNN Model\n",
        "\n",
        "After training, we evaluate the model's performance on the unseen test set. We'll look at:\n",
        "- **Test Accuracy**: The overall percentage of correct predictions.\n",
        "- **Classification Report**: A detailed breakdown of precision, recall, and F1-score for each class.\n",
        "- **Confusion Matrix**: A visual representation of the model's predictions, showing where it gets confused."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history_cnn = cnn_model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=50,  # Increased epochs, but early stopping will prevent overfitting\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Train the CNN Model\n",
        "\n",
        "We train the model using the `fit` method.\n",
        "- **Early Stopping**: We use a callback to stop training if the validation loss doesn't improve for 5 consecutive epochs. This saves time and prevents overfitting.\n",
        "- **Class Weights**: The `class_weights_dict` is passed to ensure balanced training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn_model = Sequential([\n",
        "    Conv2D(64, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(256, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    Dense(train_generator.num_classes, activation='softmax')  # Use num_classes from generator\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "cnn_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Build and Train a Custom CNN Model\n",
        "\n",
        "Now, we'll build our first model, a custom Convolutional Neural Network (CNN).\n",
        "\n",
        "### 2.1 Define the CNN Architecture\n",
        "\n",
        "The model consists of:\n",
        "- **Convolutional layers (`Conv2D`)**: To extract features like edges and textures.\n",
        "- **Batch Normalization**: To stabilize and speed up training.\n",
        "- **Max Pooling layers (`MaxPooling2D`)**: To reduce the spatial dimensions and computational load.\n",
        "- **Global Average Pooling**: To reduce the feature maps to a single vector per map.\n",
        "- **Dense layers**: For classification, with `Dropout` to prevent overfitting.\n",
        "- **Softmax activation**: In the final layer to output probabilities for each of the 9 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute class weights to handle imbalance\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_generator.classes),\n",
        "    y=train_generator.classes\n",
        ")\n",
        "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "print(\"Class weights:\", class_weights_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Handle Class Imbalance\n",
        "\n",
        "Some boat categories might have more images than others. This can bias the model towards the more frequent classes. To address this, we calculate `class_weights` to give more importance to under-represented classes during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define image size and batch size\n",
        "img_size = (224, 224)\n",
        "batch_size = 32\n",
        "\n",
        "# Data augmentation for the training set\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Only rescale for validation and test sets\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load data from directories\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Data Augmentation and Loading\n",
        "\n",
        "This section prepares the data for training. We use `ImageDataGenerator` to:\n",
        "- **Rescale** the pixel values from [0, 255] to [0, 1], which is a standard practice for neural networks.\n",
        "- **Augment** the training data by applying random transformations like rotation, zoom, and flips. This helps the model generalize better and reduces overfitting.\n",
        "- **Load** the data from the directories we created, resizing images to a standard `(224, 224)` size."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
