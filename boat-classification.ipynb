{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "KXeY5DZ97TNC",
        "outputId": "58d097b0-8720-4716-cbad-a6a7e7b1d320"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/boat_type_classification_dataset.zip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-59a658705338>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/boat_type_classification_dataset.zip'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Step 0: Extract Dataset\n",
        "zip_path = '/content/boat_type_classification_dataset.zip'\n",
        "extract_path = '/content/boat_type_classification_dataset'\n",
        "\n",
        "if not os.path.exists(extract_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content/')\n",
        "\n",
        "dataset_path = extract_path\n",
        "img_size = (224, 224)\n",
        "batch_size = 32\n",
        "\n",
        "# Step 1.1: Data Augmentation and Normalization\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Step 1.2: Load Train, Validation, and Test Data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Step 1.3: Compute Class Weights to Handle Imbalance\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_generator.classes),\n",
        "    y=train_generator.classes\n",
        ")\n",
        "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "# Step 1.4: Build Improved CNN Model\n",
        "cnn_model = Sequential([\n",
        "    Conv2D(64, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(256, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    Dense(9, activation='softmax')\n",
        "])\n",
        "\n",
        "# Step 1.5: Compile Model\n",
        "cnn_model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Step 1.6: Train CNN Model with Early Stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history_cnn = cnn_model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=50,\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Step 1.7: Evaluate on Test Set\n",
        "test_loss, test_acc = cnn_model.evaluate(test_generator)\n",
        "y_true = test_generator.classes\n",
        "y_pred = np.argmax(cnn_model.predict(test_generator), axis=1)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(classification_report(y_true, y_pred, target_names=test_generator.class_indices.keys()))\n",
        "\n",
        "# Step 1.8: Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys())\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Step 2: MobileNetV2 Model\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False  # Freeze pretrained layers\n",
        "\n",
        "mobilenet_model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(256, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.1),\n",
        "    Dense(9, activation='softmax')\n",
        "])\n",
        "\n",
        "# Step 2.6: Compile MobileNetV2 Model\n",
        "mobilenet_model.compile(\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Step 2.7: Train with Early Stopping\n",
        "history_mobilenet = mobilenet_model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=50,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Step 2.8: Evaluate on Test Set\n",
        "test_loss_mobilenet, test_acc_mobilenet = mobilenet_model.evaluate(test_generator)\n",
        "y_pred_mobilenet = np.argmax(mobilenet_model.predict(test_generator), axis=1)\n",
        "print(f\"Test Accuracy (MobileNetV2): {test_acc_mobilenet:.4f}\")\n",
        "print(classification_report(y_true, y_pred_mobilenet, target_names=test_generator.class_indices.keys()))\n",
        "\n",
        "# Step 2.9: Plot Loss & Accuracy Curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_mobilenet.history['loss'], label='Train Loss')\n",
        "plt.plot(history_mobilenet.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Train vs Validation Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_mobilenet.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_mobilenet.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Train vs Validation Accuracy')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Step 3: Compare CNN and MobileNetV2 Performance\n",
        "print(f\"CNN Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"MobileNetV2 Test Accuracy: {test_acc_mobilenet:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kdogb9Op7UI7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}