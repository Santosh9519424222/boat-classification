{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "895fd413",
   "metadata": {},
   "source": [
    "# üö¢ Boat Type Classification - Optimized Version\n",
    "\n",
    "This notebook trains a boat classification model using **MobileNetV2** with proper regularization to prevent overfitting.\n",
    "\n",
    "## What this notebook does:\n",
    "1. Splits dataset into train/validation/test (70%/15%/15%)\n",
    "2. Applies data augmentation to increase training data variety\n",
    "3. Trains MobileNetV2 with dropout and regularization\n",
    "4. Evaluates performance and saves the best model\n",
    "\n",
    "**Expected accuracy: 75-85%** (depends on dataset size and quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f27996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Required Libraries\n",
    "# =====================================\n",
    "# These libraries help us build and train the deep learning model\n",
    "\n",
    "import numpy as np                                      # For numerical operations\n",
    "import tensorflow as tf                                # Deep learning framework\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # For loading and augmenting images\n",
    "from tensorflow.keras.models import Sequential         # For building sequential model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization  # Model layers\n",
    "from tensorflow.keras.applications import MobileNetV2  # Pre-trained model\n",
    "from tensorflow.keras.optimizers import Adam           # Optimizer for training\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau  # Training callbacks\n",
    "from tensorflow.keras.regularizers import l2           # L2 regularization to prevent overfitting\n",
    "import matplotlib.pyplot as plt                        # For plotting graphs\n",
    "import seaborn as sns                                  # For better visualizations\n",
    "from sklearn.metrics import classification_report, confusion_matrix  # Performance metrics\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d937f0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split Dataset into Train/Validation/Test Sets\n",
    "# ======================================================\n",
    "# This ensures we can properly evaluate our model on unseen data\n",
    "\n",
    "base_dir = '../boat_type_classification_dataset'  # Original dataset location\n",
    "output_dir = './data'                             # Where to save split data\n",
    "\n",
    "# Create directories for each split\n",
    "train_dir = os.path.join(output_dir, 'train')\n",
    "validation_dir = os.path.join(output_dir, 'validation')\n",
    "test_dir = os.path.join(output_dir, 'test')\n",
    "\n",
    "# Remove existing data directory if it exists (to start fresh)\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "    print(\"üóëÔ∏è  Removed existing data directory\")\n",
    "\n",
    "# Create new directories\n",
    "os.makedirs(train_dir)\n",
    "os.makedirs(validation_dir)\n",
    "os.makedirs(test_dir)\n",
    "print(\"üìÅ Created train/validation/test directories\")\n",
    "\n",
    "# Get list of boat type classes (folder names)\n",
    "classes = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "print(f\"\\nüìä Found {len(classes)} boat types: {classes}\")\n",
    "\n",
    "# Split ratio (industry standard)\n",
    "train_ratio = 0.7       # 70% for training the model\n",
    "validation_ratio = 0.15  # 15% for tuning hyperparameters during training\n",
    "test_ratio = 0.15        # 15% for final evaluation (unseen data)\n",
    "\n",
    "print(f\"\\nüìà Split ratios: Train={train_ratio*100}%, Val={validation_ratio*100}%, Test={test_ratio*100}%\")\n",
    "\n",
    "# Process each boat class\n",
    "for cls in classes:\n",
    "    # Create subdirectories for this class in train/val/test\n",
    "    os.makedirs(os.path.join(train_dir, cls))\n",
    "    os.makedirs(os.path.join(validation_dir, cls))\n",
    "    os.makedirs(os.path.join(test_dir, cls))\n",
    "\n",
    "    # Get all image files for this class\n",
    "    src_dir = os.path.join(base_dir, cls)\n",
    "    all_files = [f for f in os.listdir(src_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    # Shuffle to ensure random distribution\n",
    "    random.shuffle(all_files)\n",
    "\n",
    "    # Calculate split indices\n",
    "    train_split = int(len(all_files) * train_ratio)\n",
    "    validation_split = int(len(all_files) * (train_ratio + validation_ratio))\n",
    "\n",
    "    # Split files into three sets\n",
    "    train_files = all_files[:train_split]\n",
    "    validation_files = all_files[train_split:validation_split]\n",
    "    test_files = all_files[validation_split:]\n",
    "\n",
    "    # Copy files to their respective directories\n",
    "    for f in train_files:\n",
    "        shutil.copy(os.path.join(src_dir, f), os.path.join(train_dir, cls, f))\n",
    "    for f in validation_files:\n",
    "        shutil.copy(os.path.join(src_dir, f), os.path.join(validation_dir, cls, f))\n",
    "    for f in test_files:\n",
    "        shutil.copy(os.path.join(src_dir, f), os.path.join(test_dir, cls, f))\n",
    "    \n",
    "    print(f\"   {cls:<18} Total: {len(all_files):>3} ‚Üí Train: {len(train_files):>3}, Val: {len(validation_files):>2}, Test: {len(test_files):>2}\")\n",
    "\n",
    "print(\"\\n‚úÖ Dataset split completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8c8105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Setup Data Generators with Augmentation\n",
    "# ================================================\n",
    "# Data augmentation creates variations of training images to prevent overfitting\n",
    "\n",
    "img_size = (224, 224)  # MobileNetV2 expects 224x224 images\n",
    "batch_size = 32        # Number of images processed at once (standard size)\n",
    "\n",
    "# Training data generator with augmentation\n",
    "# ==========================================\n",
    "# Augmentation helps the model learn from variations of the same image\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,              # Normalize pixel values from [0-255] to [0-1]\n",
    "    rotation_range=20,           # Randomly rotate images by up to 20 degrees\n",
    "    width_shift_range=0.15,      # Randomly shift images horizontally by 15%\n",
    "    height_shift_range=0.15,     # Randomly shift images vertically by 15%\n",
    "    shear_range=0.15,            # Apply random shearing transformations\n",
    "    zoom_range=0.15,             # Randomly zoom in/out by 15%\n",
    "    horizontal_flip=True,        # Randomly flip images horizontally (boats can face either direction)\n",
    "    fill_mode='nearest'          # Fill empty pixels with nearest neighbor\n",
    ")\n",
    "\n",
    "# Validation and test data generators (NO augmentation)\n",
    "# ======================================================\n",
    "# We only normalize validation/test data, no augmentation needed\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load data from directories\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=img_size,        # Resize all images to 224x224\n",
    "    batch_size=batch_size,       # Process 32 images at a time\n",
    "    class_mode='categorical',    # Multi-class classification (9 boat types)\n",
    "    shuffle=True                 # Shuffle training data for better learning\n",
    ")\n",
    "\n",
    "val_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False                # Don't shuffle validation data (not needed)\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False                # Don't shuffle test data\n",
    ")\n",
    "\n",
    "# Display class distribution\n",
    "print(\"\\nüìä Dataset Summary:\")\n",
    "print(f\"   Training samples: {train_generator.samples}\")\n",
    "print(f\"   Validation samples: {val_generator.samples}\")\n",
    "print(f\"   Test samples: {test_generator.samples}\")\n",
    "print(f\"   Number of classes: {train_generator.num_classes}\")\n",
    "print(f\"   Class names: {list(train_generator.class_indices.keys())}\")\n",
    "print(\"\\n‚úÖ Data generators created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cdff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Build MobileNetV2 Model with Regularization\n",
    "# ====================================================\n",
    "# We use transfer learning: take a pre-trained model and adapt it for our task\n",
    "\n",
    "print(\"üî® Building MobileNetV2 model...\\n\")\n",
    "\n",
    "# Load pre-trained MobileNetV2 (trained on ImageNet with 1.4M images)\n",
    "base_model = MobileNetV2(\n",
    "    weights='imagenet',          # Use weights pre-trained on ImageNet dataset\n",
    "    include_top=False,           # Remove the final classification layer (we'll add our own)\n",
    "    input_shape=(224, 224, 3)    # Input shape: 224x224 RGB images\n",
    ")\n",
    "\n",
    "# Freeze base model weights initially (we don't want to destroy pre-trained features)\n",
    "base_model.trainable = False\n",
    "print(\"   ‚úì Loaded pre-trained MobileNetV2\")\n",
    "print(\"   ‚úì Froze base model weights (transfer learning)\")\n",
    "\n",
    "# Build complete model by adding custom classification layers on top\n",
    "model = Sequential([\n",
    "    # Pre-trained MobileNetV2 base (feature extractor)\n",
    "    base_model,\n",
    "    \n",
    "    # Pooling layer: reduces each feature map to a single number\n",
    "    GlobalAveragePooling2D(),\n",
    "    \n",
    "    # First dense layer: 256 neurons with L2 regularization to prevent overfitting\n",
    "    Dense(256, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),        # Normalize activations for stable training\n",
    "    Dropout(0.5),                # Drop 50% of neurons randomly to prevent overfitting\n",
    "    \n",
    "    # Second dense layer: 128 neurons\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),                # Drop 30% of neurons\n",
    "    \n",
    "    # Output layer: 9 neurons (one for each boat type)\n",
    "    Dense(train_generator.num_classes, activation='softmax')  # Softmax converts to probabilities\n",
    "], name='BoatClassifier_MobileNetV2')\n",
    "\n",
    "# Compile model: specify optimizer, loss function, and metrics\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),    # Adam optimizer with low learning rate\n",
    "    loss='categorical_crossentropy',         # Loss function for multi-class classification\n",
    "    metrics=['accuracy']                     # Track accuracy during training\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "print(\"\\nüìã Model Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "# Count parameters\n",
    "trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "total_params = model.count_params()\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f\"\\nüìä Parameter Summary:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Frozen parameters: {frozen_params:,}\")\n",
    "print(\"\\n‚úÖ Model built successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f5d9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Setup Training Callbacks\n",
    "# =================================\n",
    "# Callbacks help prevent overfitting and optimize training\n",
    "\n",
    "# Early Stopping: stops training if validation loss doesn't improve\n",
    "# ===================================================================\n",
    "# Monitors validation loss and stops if no improvement for 'patience' epochs\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',          # Watch validation loss\n",
    "    patience=7,                  # Wait 7 epochs for improvement\n",
    "    restore_best_weights=True,   # Restore weights from best epoch\n",
    "    verbose=1                    # Print when stopping\n",
    ")\n",
    "\n",
    "# Learning Rate Reduction: reduces learning rate when validation loss plateaus\n",
    "# ===========================================================================\n",
    "# If validation loss stops improving, reduce learning rate to fine-tune\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',          # Watch validation loss\n",
    "    factor=0.5,                  # Reduce learning rate by 50%\n",
    "    patience=3,                  # Wait 3 epochs before reducing\n",
    "    min_lr=1e-7,                 # Don't go below this learning rate\n",
    "    verbose=1                    # Print when reducing\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training callbacks configured:\")\n",
    "print(\"   ‚úì Early Stopping (patience=7) - prevents overfitting\")\n",
    "print(\"   ‚úì Learning Rate Reduction (patience=3) - fine-tunes learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56924f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Train the Model\n",
    "# =======================\n",
    "# This is where the model learns to classify boats\n",
    "\n",
    "print(\"\\nüöÄ Starting training...\")\n",
    "print(\"=\" * 70)\n",
    "print(\"üìä Training Configuration:\")\n",
    "print(f\"   Max epochs: 50\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "print(f\"   Learning rate: 0.0001\")\n",
    "print(f\"   Training samples: {train_generator.samples}\")\n",
    "print(f\"   Validation samples: {val_generator.samples}\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n‚è±Ô∏è  This will take approximately 20-30 minutes...\\n\")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,                         # Training data\n",
    "    validation_data=val_generator,           # Validation data for monitoring\n",
    "    epochs=50,                               # Maximum number of epochs\n",
    "    callbacks=[early_stopping, reduce_lr],   # Use callbacks to prevent overfitting\n",
    "    verbose=1                                # Show progress bar\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")\n",
    "print(f\"   Total epochs trained: {len(history.history['accuracy'])}\")\n",
    "print(f\"   Final training accuracy: {history.history['accuracy'][-1]*100:.2f}%\")\n",
    "print(f\"   Final validation accuracy: {history.history['val_accuracy'][-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da8c192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Visualize Training History\n",
    "# ===================================\n",
    "# Plot accuracy and loss curves to diagnose training\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Accuracy over epochs\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "plt.title('Model Accuracy Over Time', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Loss over epochs\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "plt.title('Model Loss Over Time', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpretation guide\n",
    "print(\"\\nüìä How to Read These Plots:\")\n",
    "print(\"\\n‚úÖ Good Signs (Model is learning well):\")\n",
    "print(\"   ‚Ä¢ Training and validation curves are close together\")\n",
    "print(\"   ‚Ä¢ Both accuracies increase over time\")\n",
    "print(\"   ‚Ä¢ Both losses decrease over time\")\n",
    "print(\"\\n‚ö†Ô∏è  Warning Signs (Overfitting):\")\n",
    "print(\"   ‚Ä¢ Large gap between training and validation accuracy\")\n",
    "print(\"   ‚Ä¢ Training loss keeps decreasing but validation loss increases\")\n",
    "print(\"   ‚Ä¢ If you see this, the model memorized training data instead of learning patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd69e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Evaluate on Test Set\n",
    "# =============================\n",
    "# Test on completely unseen data to get true performance\n",
    "\n",
    "print(\"üìä Evaluating model on test set...\\n\")\n",
    "\n",
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(test_generator, verbose=1)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéØ FINAL TEST RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"   Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"   Test Loss: {test_loss:.4f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Interpretation\n",
    "if test_acc >= 0.80:\n",
    "    print(\"\\n‚úÖ EXCELLENT! Accuracy above 80% - Model is ready for deployment!\")\n",
    "elif test_acc >= 0.70:\n",
    "    print(\"\\n‚úÖ GOOD! Accuracy above 70% - Model works well, can be improved with more data\")\n",
    "elif test_acc >= 0.60:\n",
    "    print(\"\\n‚ö†Ô∏è  FAIR! Accuracy above 60% - Model needs more training data or tuning\")\n",
    "else:\n",
    "    print(\"\\n‚ùå POOR! Accuracy below 60% - Consider collecting more data or trying different architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79a2de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Detailed Performance Analysis\n",
    "# ======================================\n",
    "# Generate predictions and analyze per-class performance\n",
    "\n",
    "print(\"üîç Generating predictions for detailed analysis...\\n\")\n",
    "\n",
    "# Get predictions for all test images\n",
    "y_pred = np.argmax(model.predict(test_generator), axis=1)\n",
    "y_true = test_generator.classes\n",
    "class_labels = list(train_generator.class_indices.keys())\n",
    "\n",
    "# Classification Report: shows precision, recall, F1-score for each class\n",
    "print(\"üìã Classification Report:\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(y_true, y_pred, target_names=class_labels))\n",
    "\n",
    "print(\"\\nüí° Understanding the Metrics:\")\n",
    "print(\"   ‚Ä¢ Precision: Of all predictions for this class, how many were correct?\")\n",
    "print(\"   ‚Ä¢ Recall: Of all actual images of this class, how many did we find?\")\n",
    "print(\"   ‚Ä¢ F1-Score: Harmonic mean of precision and recall (overall quality)\")\n",
    "print(\"   ‚Ä¢ Support: Number of test images for this class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6370110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Confusion Matrix Visualization\n",
    "# ========================================\n",
    "# See which classes the model confuses with each other\n",
    "\n",
    "print(\"üìä Creating confusion matrix...\\n\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    conf_matrix, \n",
    "    annot=True,              # Show numbers in cells\n",
    "    fmt='d',                 # Format as integers\n",
    "    cmap='Blues',            # Color scheme\n",
    "    xticklabels=class_labels,\n",
    "    yticklabels=class_labels,\n",
    "    cbar_kws={'label': 'Number of Images'}\n",
    ")\n",
    "plt.xlabel('Predicted Class', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Actual Class', fontsize=12, fontweight='bold')\n",
    "plt.title(f'Confusion Matrix - Test Accuracy: {test_acc*100:.2f}%', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° How to Read the Confusion Matrix:\")\n",
    "print(\"   ‚Ä¢ Diagonal (top-left to bottom-right): Correct predictions\")\n",
    "print(\"   ‚Ä¢ Off-diagonal: Misclassifications\")\n",
    "print(\"   ‚Ä¢ Darker colors = more images in that cell\")\n",
    "print(\"   ‚Ä¢ Perfect model would have ALL numbers on the diagonal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d89d04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Save the Trained Model\n",
    "# ================================\n",
    "# Save model to use in the web application\n",
    "\n",
    "model_filename = 'boat_classifier_mobilenet.h5'\n",
    "model.save(model_filename)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üíæ MODEL SAVED SUCCESSFULLY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"   Filename: {model_filename}\")\n",
    "print(f\"   Location: {os.path.abspath(model_filename)}\")\n",
    "print(f\"   File size: {os.path.getsize(model_filename) / (1024*1024):.2f} MB\")\n",
    "print(\"\\nüìù Next Steps:\")\n",
    "print(\"   1. Move the model to backend folder:\")\n",
    "print(f\"      Move-Item -Path '{model_filename}' -Destination 'backend/{model_filename}' -Force\")\n",
    "print(\"   2. Start the backend server:\")\n",
    "print(\"      cd backend; python app.py\")\n",
    "print(\"   3. Open frontend/index.html in your browser\")\n",
    "print(\"   4. Test with boat images!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe72e7bf",
   "metadata": {},
   "source": [
    "## üéâ Training Complete!\n",
    "\n",
    "### Summary of What We Did:\n",
    "\n",
    "1. ‚úÖ Split dataset (70% train, 15% validation, 15% test)\n",
    "2. ‚úÖ Applied data augmentation to prevent overfitting\n",
    "3. ‚úÖ Built MobileNetV2 model with regularization\n",
    "4. ‚úÖ Trained with early stopping and learning rate reduction\n",
    "5. ‚úÖ Evaluated on unseen test data\n",
    "6. ‚úÖ Analyzed per-class performance\n",
    "7. ‚úÖ Saved model for deployment\n",
    "\n",
    "### How to Improve Accuracy Further:\n",
    "\n",
    "1. **Collect More Data** (Most Important!)\n",
    "   - Classes with few images (<20) will perform poorly\n",
    "   - Aim for 100+ images per class for best results\n",
    "\n",
    "2. **Try Fine-Tuning**\n",
    "   - Unfreeze top layers of MobileNetV2 and train with lower learning rate\n",
    "\n",
    "3. **Try Different Architectures**\n",
    "   - EfficientNetB3: Better accuracy but slower\n",
    "   - ResNet50: Good for complex features\n",
    "   - InceptionV3: Good for varied image sizes\n",
    "\n",
    "### Expected Performance:\n",
    "- **With balanced data (100+ images/class)**: 85-90% accuracy\n",
    "- **With imbalanced data**: 70-80% accuracy\n",
    "- **Production systems typically achieve**: 75-85% accuracy"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
